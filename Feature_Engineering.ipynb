{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Engineering Asssignment"
      ],
      "metadata": {
        "id": "OGmvvEPSFQ3E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Que1. What is a parameter?\n",
        "\n",
        "Ans: A parameter in this context is a configurable setting that affects how features are constructed, scaled, or encoded. Unlike model parameters (e.g., weights in a neural network), which are learned during training, feature engineering parameters are set manually or through experimentation."
      ],
      "metadata": {
        "id": "lxC9nnTgFT17"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Que2. What is correlation? What does negative correlation mean?\n",
        "\n",
        "Ans: Correlation is a statistical measure that describes the strength and direction of a relationship between two variables. It indicates how changes in one variable are associated with changes in another, without implying causation.\n",
        "\n",
        "Negative Correlation: When one variable increases, the other decreases. For instance, as the amount of exercise increases, body weight may decrease."
      ],
      "metadata": {
        "id": "fceXZCHCIyBy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Que3. Define Machine Learning. What are the main components in Machine Learning?\n",
        "\n",
        "Ans: Machine Learning (ML) is a subset of Artificial Intelligence (AI) that enables computers to learn from data and improve their performance over time without being explicitly programmed. Instead of following predefined rules, ML algorithms identify patterns in data and make decisions or predictions based on that information.\n",
        "\n",
        "* Main Components of Machine Learning.\n",
        "\n",
        "A typical machine learning system comprises several key components that work together to process data and generate insights:\n",
        "\n",
        "1. Data\n",
        "\n",
        "Data serves as the foundation of any ML model. It encompasses raw information in various forms, such as text, images, audio, or numerical values. The quality and quantity of data significantly influence the model's performance.\n",
        "\n",
        "2. Features\n",
        "\n",
        "Features are individual measurable properties or characteristics of the data. For example, in a dataset predicting house prices, features might include the number of bedrooms, square footage, or location. Selecting relevant features is crucial for building effective models.\n",
        "\n",
        "3. Algorithms\n",
        "\n",
        "Algorithms are mathematical models or procedures that process data to identify patterns and make predictions. Common ML algorithms include linear regression, decision trees, and neural networks. The choice of algorithm depends on the specific problem and data characteristics.\n",
        "\n",
        "4. Model\n",
        "\n",
        "A model is the output of an ML algorithm after it has been trained on data. It represents the learned patterns and is used to make predictions on new, unseen data.\n",
        "\n",
        "5. Training\n",
        "\n",
        "Training involves feeding data into an algorithm to allow the model to learn from it. During this phase, the model adjusts its parameters to minimize errors and improve accuracy."
      ],
      "metadata": {
        "id": "QvvNrqlZJfJC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Que4. How does loss value help in determining whether the model is good or not?\n",
        "\n",
        "Ans: The loss value provides insights into the model's accuracy:\n",
        "\n",
        "Low Loss: Indicates that the model's predictions are close to the actual values, suggesting good performance.\n",
        "\n",
        "High Loss: Signifies that the model's predictions deviate significantly from the actual values, indicating poor performance.\n",
        "\n",
        "For instance, in regression tasks, a lower MSE indicates that the model's predictions are closer to the true values.\n",
        "\n"
      ],
      "metadata": {
        "id": "7TkAW5IkKHum"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Que5. What are continuous and categorical variables?\n",
        "\n",
        "Ans: 1) Continuous variables are quantitative variables that can take on an infinite number of values within a given range. They are measurable and can be expressed in fractions or decimals.\n",
        "\n",
        "Characteristics:\n",
        "\n",
        "Can assume an infinite number of values within a specified range.\n",
        "\n",
        "Measured on a continuous scale.\n",
        "\n",
        "Can be subjected to arithmetic operations (addition, subtraction, etc.).\n",
        "\n",
        "Often analyzed using statistical methods that assume normal distribution (e.g., t-tests, ANOVA).\n",
        "\n",
        "2) Categorical variables (also known as qualitative variables) represent distinct categories or groups and do not have a numerical value. They can be further divided into:\n",
        "\n",
        "\n",
        "Nominal variables: Categories without inherent order.\n",
        "\n",
        "Ordinal variables: Categories with a defined order or ranking.\n",
        "\n",
        "Characteristics:\n",
        "\n",
        "Represent distinct categories or groups.\n",
        "\n",
        "Cannot be subjected to arithmetic operations in a meaningful way.\n",
        "\n",
        "Analyzed using methods suitable for categorical data, such as Chi-Square tests or logistic regression."
      ],
      "metadata": {
        "id": "cOfnGyOfz-C3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Que6. How do we handle categorical variables in Machine Learning? What are the common techniques?\n",
        "\n",
        "Ans: Handling categorical variables effectively is crucial in machine learning, as most algorithms require numerical input. Several encoding techniques are employed to transform categorical data into a format suitable for model training. Here's an overview of the most common methods:\n",
        "\n",
        "1.  One-Hot Encoding:\n",
        "\n",
        "This technique creates a binary column for each category in the original variable. Each observation is marked with a 1 in the column corresponding to its category and 0 in all others.\n",
        "\n",
        "Use case: Ideal for nominal variables without any ordinal relationship.\n",
        "\n",
        "Pros: Prevents the model from assuming any ordinal relationship between categories.\n",
        "\n",
        "Cons: Can lead to high-dimensional data, especially with variables having many unique categories.\n",
        "\n",
        "2. Ordinal Encoding :\n",
        "\n",
        "Description: Assigns integer values to categories based on their order.\n",
        "\n",
        "Use Case: Suitable for ordinal data where the categories have a meaningful order but not necessarily evenly spaced (e.g., 'Poor', 'Average', 'Good').\n",
        "\n",
        "3. Label Encoding :\n",
        "\n",
        "Description: Assigns a unique integer to each category.\n",
        "\n",
        "Use Case: Suitable for ordinal data where categories have a meaningful order (e.g., 'Low', 'Medium', 'High').\n",
        "\n",
        "4. Frequency Encoding :\n",
        "\n",
        "Description: Replaces categories with their frequency of occurrence in the dataset.\n",
        "\n",
        "Use Case: Useful when the frequency of categories carries predictive information.\n",
        "\n",
        "5.  Target Encoding (Mean Encoding) :\n",
        "\n",
        "Description: Replaces categories with the mean of the target variable for each category.\n",
        "\n",
        "Use Case: Effective when there is a strong relationship between the categorical feature and the target variable.\n",
        "\n",
        "6. Binary Encoding :\n",
        "\n",
        "Description: Converts categories into binary numbers and splits the digits into separate columns.\n",
        "\n",
        "Use Case: Suitable for high-cardinality categorical variables.\n",
        "\n",
        "7. Embedding Layers (Deep Learning) :\n",
        "\n",
        "Description: Maps categories to dense vectors in a continuous vector space.\n",
        "\n",
        "Use Case: Particularly useful for high-cardinality categorical variables in deep learning models.\n",
        "\n",
        "Consideration: Requires larger datasets and more computational resources."
      ],
      "metadata": {
        "id": "7wcvEWDW00MP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Que7. What do you mean by training and testing a dataset?\n",
        "\n",
        "Ans:  \n",
        "1. Training Dataset :\n",
        "\n",
        "Purpose: Used to train the machine learning model. The model learns from this data by identifying patterns and relationships between input features and target labels.\n",
        "\n",
        "Characteristics:\n",
        "\n",
        "a) Typically comprises 70â€“80% of the total dataset.\n",
        "\n",
        "b) Contains labeled data (input-output pairs).\n",
        "\n",
        "Enables the model to adjust its parameters to minimize errors.\n",
        "\n",
        "c) Analogy: Similar to a student studying from textbooks to understand a subject.\n",
        "\n",
        "Testing Dataset :\n",
        "\n",
        "Purpose: Evaluates the performance of the trained model on unseen data. It helps assess how well the model generalizes to new, real-world data.\n",
        "\n",
        "Characteristics:\n",
        "\n",
        "a) Usually accounts for 20â€“30% of the total dataset.\n",
        "\n",
        "b) Not used during the training process.\n",
        "\n",
        "c) Provides an unbiased evaluation of the model's accuracy and effectiveness.\n",
        "\n",
        "\n",
        "Analogy: Comparable to a student taking an exam to demonstrate their understanding.\n"
      ],
      "metadata": {
        "id": "01nBWXvV17_x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Que8. What is sklearn.preprocessing?\n",
        "\n",
        "Ans: The sklearn.preprocessing module in scikit-learn is a collection of utilities designed to prepare and transform raw data into formats suitable for machine learning models. Effective preprocessing can significantly enhance model performance, especially when dealing with real-world datasets that often require scaling, encoding, or normalization."
      ],
      "metadata": {
        "id": "zOvnS1M91y7y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Que9. What is a Test set?\n",
        "\n",
        "Ans: In machine learning, a test set is a subset of your dataset that is used exclusively to evaluate the performance of a trained model. It serves as an unbiased benchmark to assess how well the model generalizes to new, unseen data."
      ],
      "metadata": {
        "id": "64W_fV_5-XqJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Que10. How do we split data for model fitting (training and testing) in Python? How do you approach a Machine Learning problem?\n",
        "\n",
        "#Ans: In Python, the train_test_split() function from the sklearn.model_selection module is commonly used to divide datasets into training and testing subsets. Here's how you can do it:\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "# Load your dataset\n",
        "df = pd.read_csv('your_dataset.csv')\n",
        "\n",
        "# Separate features (X) and target (y)\n",
        "X = df.drop('target_column', axis=1)\n",
        "y = df['target_column']\n",
        "\n",
        "# Split the data\n",
        "\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "# test_size=0.2: Allocates 20% of the data to the test set, leaving 80% for training.\n",
        "\n",
        "# random_state=42: Ensures reproducibility by setting a seed for random number generation.\n",
        "# GeeksforGeeks\n",
        "\n",
        "# This method provides a straightforward way to split your data and is widely used in machine learning workflows"
      ],
      "metadata": {
        "id": "UZJC_r5mBEXg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " * How to Approach a Machine Learning Problem :\n",
        " Approaching a machine learning problem systematically is crucial for building effective models. Here's a structured approach:\n",
        "\n",
        "1. Define the Problem: Clearly understand and articulate the problem you're trying to solve.\n",
        "\n",
        "3. Collect and Prepare Data:\n",
        "\n",
        " Data Collection: Gather relevant data from various sources.\n",
        "\n",
        " Data Cleaning: Handle missing values, remove duplicates, and correct errors.\n",
        "\n",
        " Feature Engineering: Create new features that can improve model performance.\n",
        "\n",
        "7. Choose a Model: Select an appropriate machine learning algorithm based on the problem type (e.g., classification, regression).\n",
        "\n",
        "8. Train the Model: Use the training data to train the model.\n",
        "Lifewire\n",
        "\n",
        "9. Evaluate the Model: Assess the model's performance using the test data and appropriate metrics (e.g., accuracy, precision, recall).\n",
        "\n",
        "10. Tune Hyperparameters: Optimize the model's hyperparameters to improve performance.\n",
        "\n",
        "11. Deploy the Model: Implement the model in a real-world environment for making predictions.\n",
        " This approach ensures a comprehensive and structured process for tackling machine learning problems."
      ],
      "metadata": {
        "id": "Wgfi8HW7B9nw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Que11. Why do we have to perform EDA before fitting a model to the data?\n",
        "\n",
        "Ans: Why Perform EDA Before Model Fitting.\n",
        "\n",
        "1. Understanding Data Structure and Distribution:\n",
        "\n",
        "EDA helps in comprehending the dataset's structure, including the number of features, their types (numerical or categorical), and how data is distributed. This understanding is vital for selecting the appropriate machine learning algorithms and preprocessing techniques.\n",
        "\n",
        "\n",
        "2. Identifying and Handling Missing Values\n",
        "\n",
        "Missing data can significantly impact model performance. Through EDA, you can detect missing values and decide on suitable strategies to handle them, such as imputation or removal, ensuring the integrity of the dataset.\n",
        "\n",
        "3. Detecting Outliers and Anomalies\n",
        "\n",
        "Outliers can skew model results and lead to overfitting. EDA techniques like box plots and scatter plots help in identifying these anomalies, allowing for their treatment or removal before modeling.\n",
        "\n",
        "4. Assessing Feature Relationships\n",
        "\n",
        "Understanding correlations between features can inform feature selection and engineering. EDA reveals how variables relate to each other, aiding in the identification of redundant or highly correlated features that may need to be addressed.\n",
        "\n",
        "5. Testing Assumptions\n",
        "\n",
        "Many machine learning algorithms have underlying assumptions, such as normality of data. EDA allows you to test these assumptions, guiding necessary transformations or adjustments to the data.\n",
        "Applied AI Course\n",
        "\n",
        "6. Informing Feature Engineering\n",
        "\n",
        "Insights gained from EDA can inspire new features or transformations that enhance model performance. For instance, recognizing non-linear relationships might lead to the creation of interaction terms or polynomial features.\n",
        "\n",
        "7. Improving Model Interpretability\n",
        "\n",
        "A thorough understanding of the data through EDA ensures that the model's behavior is interpretable. Clean and well-understood data contribute to clearer relationships between features and outcomes."
      ],
      "metadata": {
        "id": "RrluuVR7DBFn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Que12. What is correlation?\n",
        "\n",
        "Ans: Correlation is a statistical measure that indicates the extent to which two variables fluctuate in relation to each other. It assesses the strength and direction of a linear relationship between variables, providing insights into how one variable may change in response to another."
      ],
      "metadata": {
        "id": "7h-j47hkE5Al"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Que13. What does negative correlation mean?\n",
        "\n",
        "Ans: Negative correlation describes a statistical relationship between two variables where, as one variable increases, the other tends to decrease, and vice versa. This inverse relationship is quantified using a correlation coefficient, typically denoted as r, which ranges from -1 to +1. A negative correlation is indicated by values between 0 and -1, with -1 representing a perfect negative correlation.\n",
        "\n"
      ],
      "metadata": {
        "id": "v-Tv4qMfFOc1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Que14. How can you find correlation between variables in Python?\n",
        "\n",
        "Ans: To compute the correlation between variables in Python, particularly using the pandas library, you can follow these steps:\n",
        "\n"
      ],
      "metadata": {
        "id": "oKftCcwQF64U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Sample DataFrame\n",
        "data = {'A': [3, 2, 1],\n",
        "        'B': [4, 6, 5],\n",
        "        'C': [7, 18, 91]}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Compute the correlation matrix\n",
        "correlation_matrix = df.corr()\n",
        "\n",
        "print(correlation_matrix)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2QXJlRo7GtDm",
        "outputId": "b59d8da6-43dd-401f-94a2-816a6892fcaf"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          A        B         C\n",
            "A  1.000000 -0.50000 -0.919953\n",
            "B -0.500000  1.00000  0.120470\n",
            "C -0.919953  0.12047  1.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Que15. What is causation? Explain difference between correlation and causation with an example.\n",
        "\n",
        "Ans: Causation refers to a cause-and-effect relationship where one event (the cause) directly influences another event (the effect). In contrast, correlation indicates a statistical association between two variables, but it doesn't imply that one causes the other.\n",
        "\n",
        "* Difference Between Correlation and Causation\n",
        "\n",
        "1) Definition\n",
        "\n",
        "* Correlation : Measures the strength and direction of a relationship between two variables.\n",
        "\n",
        "*  Causation :\n",
        "Indicates that one event is the direct result of another.\n",
        "\n",
        "2) Implication\n",
        "\n",
        "*   Correlation : Does not imply that one variable causes the other to change.\n",
        "\n",
        "* Causation : Implies a direct cause-and-effect relationship.\n",
        "\n",
        "3) Example :\n",
        "\n",
        "* Correlation : Ice cream sales and drowning incidents are correlated; both increase during summer.\n",
        "\n",
        "*  Causation : Smoking causes an increase in the risk of developing lung cancer."
      ],
      "metadata": {
        "id": "2VitcpvfG7Tt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Que16. What is an Optimizer? What are different types of optimizers? Explain each with an example.\n",
        "\n",
        "Ans: In machine learning and deep learning, an optimizer is an algorithm used to adjust the weights and biases of a model during training to minimize the loss function. The goal is to find the optimal parameters that lead to the best performance of the model.\n",
        "\n",
        "Common Types of Optimizers :\n",
        "\n",
        "1. Stochastic Gradient Descent (SGD) :\n",
        "\n",
        "Description: SGD updates the model parameters using the gradient of the loss function with respect to the weights, computed on a single data point.\n",
        "\n",
        "Use Case: Suitable for large datasets and online learning scenarios.\n",
        "\n",
        "Example: Training a neural network on a large image dataset where data is streamed in batches.\n",
        "\n",
        "2. Momentum\n",
        "\n",
        "Description: Momentum builds upon SGD by adding a fraction of the previous update to the current one, helping to accelerate convergence and reduce oscillations.\n",
        "\n",
        "Use Case: Effective in navigating ravines in error surfaces, common in deep learning models.\n",
        "\n",
        "Example: Training deep neural networks where the loss surface has steep gradients.\n",
        "\n",
        "3. RMSprop (Root Mean Square Propagation) :\n",
        "\n",
        "Description: RMSprop divides the learning rate by an exponentially decaying average of squared gradients, adapting the learning rate for each parameter.\n",
        "\n",
        "Use Case: Works well in non-stationary settings, such as training recurrent neural networks.\n",
        "\n",
        "Example: Training models on time-series data where the data distribution changes over time.\n",
        "\n",
        "\n",
        "4. Adam (Adaptive Moment Estimation) :\n",
        "\n",
        "Description: Adam combines the advantages of both Momentum and RMSprop by using both first and second moments of the gradients to adaptively adjust the learning rates.\n",
        "\n",
        "Use Case: Widely used in various deep learning tasks due to its efficiency and ease of use.\n",
        "\n",
        "Example: Training large-scale models like GPT-3 and BERT.\n",
        "Medium\n",
        "\n",
        "5. AdaGrad (Adaptive Gradient Algorithm) :\n",
        "\n",
        "Description: AdaGrad adapts the learning rate for each parameter based on the frequency of updates, assigning smaller learning rates to frequently updated parameters.\n",
        "\n",
        "Use Case: Effective for sparse data scenarios, such as text classification tasks.\n",
        "\n",
        "Example: Training models on datasets with many rare features.\n",
        "Medium\n",
        "\n",
        "6. AdaDelta\n",
        "\n",
        "Description: AdaDelta is an extension of AdaGrad that seeks to solve the problem of diminishing learning rates by using a moving average of squared gradients.\n",
        "\n",
        "Use Case: Suitable for tasks where the learning rate needs to be adjusted dynamically.\n",
        "\n",
        "Example: Training models on datasets with varying feature scales.\n",
        "Medium\n",
        "\n",
        "7. Nadam (Nesterov-accelerated Adaptive Moment Estimation)\n",
        "\n",
        "Description: Nadam combines Adam with Nesterov momentum, allowing the optimizer to look ahead of the current parameter update.\n",
        "\n",
        "Use Case: Effective in training deep networks where training is slow due to the vanishing gradient problem.\n",
        "\n",
        "Example: Fine-tuning large pre-trained models like BERT."
      ],
      "metadata": {
        "id": "FtsMQWHsJlvH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Que17. What is sklearn.linear_model ?\n",
        "\n",
        "Ans: sklearn.linear_model is a module within the scikit-learn library in Python that provides a variety of linear models for both regression and classification tasks. These models assume that the target variable is a linear combination of the input features, making them foundational tools in machine learning.\n",
        "\n"
      ],
      "metadata": {
        "id": "UFseMk14Nser"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Que18. What does model.fit() do? What arguments must be given?\n",
        "\n",
        "Ans: What Does model.fit() Do.\n",
        "\n",
        "The fit() method trains the model by:\n",
        "\n",
        "\n",
        "Supervised Learning: Learning the relationship between input features (X) and target labels (y).\n",
        "\n",
        "Unsupervised Learning: Identifying patterns or structures in the input data (X) without predefined labels.\n",
        "\n",
        "During this process, the model computes and stores parameters such as coefficients, centroids, or cluster assignments, depending on the algorithm used.\n",
        "\n",
        "Required Arguments :\n",
        "\n",
        "For supervised learning models, fit() requires:\n",
        "\n",
        "X: Feature matrix (2D array-like) of shape (n_samples, n_features).\n",
        "\n",
        "y: Target vector (1D array-like) of shape (n_samples,).\n",
        "Lxadm.com\n",
        "\n",
        "For unsupervised learning models, only X is needed:\n",
        "\n",
        "\n",
        "X: Feature matrix (2D array-like) of shape (n_samples, n_features).\n",
        "\n",
        "It's crucial that X and y have compatible shapes, specifically that X.shape[0] == y.shape[0] for supervised tasks."
      ],
      "metadata": {
        "id": "dA2VK3JIOpQ7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Que19. What does model.predict() do? What arguments must be given?\n",
        "\n",
        "Ans: The predict() method generates predictions based on the patterns the model has learned during training. For regression tasks, it outputs continuous values, while for classification tasks, it provides predicted class labels.\n",
        "\n",
        " Required Arguments :\n",
        "\n",
        "1) The predict() method requires a single argument:\n",
        "\n",
        "2) X: Feature matrix (2D array-like) of shape (n_samples, n_features) representing the new input data for which predictions are to be made.\n",
        "\n"
      ],
      "metadata": {
        "id": "qO6BUwpmPwVJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Que20. What are continuous and categorical variables?\n",
        "\n",
        "Ans: 1) Continuous variables are numerical variables that can take on an infinite number of values within a given range. They are measurable and can be expressed in fractions or decimals.\n",
        "\n",
        "\n",
        "ðŸ”¹ Characteristics:\n",
        "Infinite Possibilities: Can assume an infinite number of values within a range.\n",
        "\n",
        "Measurable: Represent quantities that can be measured.\n",
        "\n",
        "Arithmetic Operations: Allow for meaningful arithmetic operations (e.g., addition, subtraction).\n",
        "\n",
        " Examples:\n",
        "\n",
        "Height: A person's height can be 170.5 cm, 170.55 cm, etc.\n",
        "\n",
        "Weight: A person's weight can be 65.2 kg, 65.25 kg, etc.\n",
        "\n",
        "2. Categorical variables represent distinct categories or groups and do not have a numerical value. They can be further divided into:\n",
        "\n",
        "Nominal Variables: Categories without a natural order.\n",
        "\n",
        "Ordinal Variables: Categories with a meaningful order or ranking.\n",
        "Social Science Computing Cooperative\n",
        "\n",
        "\n",
        "ðŸ”¹ Characteristics:\n",
        "\n",
        "Limited Categories: Contain a finite number of categories or distinct groups.\n",
        "\n",
        "Non-Numeric: Represent qualitative attributes.\n",
        "\n",
        "Non-Arithmetic: Arithmetic operations are not meaningful.\n",
        "\n",
        "\n",
        " Examples:\n",
        "\n",
        "1. Nominal:\n",
        "\n",
        "Gender: Male, Female, Other\n",
        "\n",
        "Blood Type: A, B, AB, O\n",
        "\n",
        "2. Ordinal:\n",
        "\n",
        "Education Level: High School, Bachelor's, Master's, PhD\n",
        "\n",
        "Satisfaction Rating: Very Unsatisfied, Unsatisfied, Neutral, Satisfied, Very Satisfied\n",
        "\n",
        "Categorical variables are analyzed using methods suitable for categorical data, such as Chi-Square tests or logistic regression ."
      ],
      "metadata": {
        "id": "xwT8aUftQhis"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Que21. What is feature scaling? How does it help in Machine Learning?\n",
        "\n",
        "Ans: Feature scaling is a data preprocessing technique in machine learning that involves transforming the features (independent variables) of a dataset so that they are on a similar scale. This step is crucial because many machine learning algorithms perform better or converge faster when features are on a similar scale.\n",
        "\n",
        " * Why is Feature Scaling Important.\n",
        "\n",
        "1. Improved Algorithm Performance: Algorithms like gradient descent-based models (e.g., linear regression, logistic regression, neural networks) and distance-based algorithms (e.g., K-Nearest Neighbors, Support Vector Machines) are sensitive to the scale of input features. Without scaling, features with larger ranges can dominate the learning process, leading to biased models.\n",
        "\n",
        "\n",
        "2. Faster Convergence: For optimization algorithms like gradient descent, feature scaling can lead to faster convergence by ensuring that the gradient steps are uniform across all features.\n",
        "\n",
        "3. Prevention of Numerical Instability: Large differences in feature scales can cause numerical instability during model training, leading to errors or suboptimal performance.\n",
        "BytePlus\n",
        "\n",
        "4. Equal Feature Contribution: Scaling ensures that each feature contributes equally to the model, preventing features with larger numerical ranges from disproportionately influencing the model's predictions"
      ],
      "metadata": {
        "id": "EaeDQB57XIq5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Que22. How do we perform scaling in Python?\n",
        "\n",
        "Ans: In Python, feature scaling is commonly performed using the scikit-learn library, which provides several preprocessing tools to standardize or normalize your data. Here's how you can apply different scaling techniques:\n",
        "\n",
        "1. Standardization (Z-score Normalization)\n",
        "Standardization transforms your data to have a mean of 0 and a standard deviation of 1, making it suitable for algorithms like Support Vector Machines (SVM), K-Nearest Neighbors (KNN), and neural networks.\n",
        "\n",
        "2. Min-Max Scaling\n",
        "Min-Max Scaling rescales the data to a fixed range, typically [0, 1]. This is useful when you need a bounded range, such as for neural networks.\n",
        "\n",
        "3. Robust Scaling\n",
        "Robust Scaling uses the median and the interquartile range to scale features, making it robust to outliers.\n",
        "\n"
      ],
      "metadata": {
        "id": "PvWq5OngZY2z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Que23. What is sklearn.preprocessing?\n",
        "\n",
        "Ans: The sklearn.preprocessing module in scikit-learn provides a suite of tools for data preprocessing, which is a crucial step in the machine learning pipeline. These tools help transform raw data into a format suitable for modeling, ensuring that algorithms perform optimally.\n",
        "\n"
      ],
      "metadata": {
        "id": "HxFB6NaXcK1_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Que24. How do we split data for model fitting (training and testing) in Python?\n",
        "\n",
        "Ans: The train_test_split function randomly splits arrays or matrices into training and testing subsets. Here's how to use it:\n",
        "\n"
      ],
      "metadata": {
        "id": "oKw7N4FHdbq9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Sample data\n",
        "X = [[1, 2], [3, 4], [5, 6], [7, 8]]\n",
        "y = [0, 1, 0, 1]\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "\n",
        "print(\"Training features:\", X_train)\n",
        "print(\"Testing features:\", X_test)\n",
        "print(\"Training labels:\", y_train)\n",
        "print(\"Testing labels:\", y_test)\n"
      ],
      "metadata": {
        "id": "6HtUTD2zernN",
        "outputId": "a39df8f5-cc43-446f-8f6f-0fec25a1579c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training features: [[7, 8], [1, 2], [5, 6]]\n",
            "Testing features: [[3, 4]]\n",
            "Training labels: [1, 0, 0]\n",
            "Testing labels: [1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Que25. Explain data encoding?\n",
        "\n",
        "Ans: Data encoding is a crucial step in machine learning that involves converting categorical variablesâ€”such as \"Color\" or \"Gender\"â€”into numerical formats that algorithms can process effectively. Since most machine learning models require numerical input, encoding ensures that categorical data can be utilized appropriately.\n",
        "\n",
        "1. Label Encoding\n",
        "Assigns each unique category a numerical label.\n",
        "\n",
        "Use Case: Suitable for ordinal data where the order matters (e.g., \"Low\", \"Medium\", \"High\").\n",
        "\n",
        "2. One-Hot Encoding :\n",
        "\n",
        "Creates a new binary column for each category, indicating the presence of each category with 1 or 0.\n",
        "\n",
        "Use Case: Ideal for nominal data where no ordinal relationship exists (e.g., \"Red\", \"Green\", \"Blue\").\n",
        "\n",
        "3. Ordinal Encoding\n",
        "Assigns integers to categories based on a predefined order.\n",
        "\n",
        "Use Case: Best for ordinal data where the categories have a meaningful order (e.g., \"Low\", \"Medium\", \"High\").\n",
        "\n",
        "4. Binary Encoding\n",
        "Converts categories into binary code and splits the digits into separate columns.\n",
        "Medium\n",
        "\n",
        "Use Case: Useful for high-cardinality features to reduce dimensionality.\n",
        "\n",
        "5. Frequency Encoding\n",
        "Replaces categories with their frequency of occurrence in the dataset.\n",
        "\n",
        "Use Case: Effective for high-cardinality features where one-hot encoding may lead to sparse matrices."
      ],
      "metadata": {
        "id": "V0oRRUIlfPD8"
      }
    }
  ]
}